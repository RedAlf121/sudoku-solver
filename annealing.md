# ğŸ”¥ Simulated Annealing

El recocido o temple simulado es un algoritmo de optimizaciÃ³n âš™ï¸. Es una forma de buscar la mejor soluciÃ³n cuando no se tiene una forma de calcular de forma lineal ğŸ“‰.

Este algoritmo estÃ¡ inspirado en un proceso para trabajar metales llamado Annealing ğŸ§ŠğŸ”©.

## ğŸ§ª Contexto desde la fÃ­sica

Cuando se quiere eliminar tensiones de un metal, se hace lo siguiente:  
Se calienta el metal a una alta temperatura ğŸŒ¡ï¸ (sin caer en la temperatura de fusiÃ³n que lo derrite)  
Y luego se va dejando enfriar poco a poco â„ï¸ hasta que llega a temperatura ambiente ğŸŒ¬ï¸.  
Eso hace que el metal sea maleable y se faciliten procesos en frÃ­o como cortar âœ‚ï¸, doblar ğŸ”§, etc.  
En teorÃ­a, el metal estarÃ­a en su estado Ã³ptimo âœ….

### â“ Â¿Y por quÃ© ocurre eso?

Sucede que el metal, al estar a altas temperaturas, los Ã¡tomos del metal se empiezan a mover de forma arbitraria ğŸ”€.  
Al irse enfriando comienzan a agruparse ğŸ§² ya que el aumento de la temperatura no es 100% parejo.  
Por la propia forma del metal, no son exactamente iguales las temperaturas.  
Como no son iguales, hay Ã¡tomos que se van a enfriar mÃ¡s rÃ¡pido y se van a agrupar con otros, lo que crea una estructura que le dicen granos cristalinos ğŸ§Šâœ¨.

### ğŸ’» Â¿Y ahora quÃ© tiene que ver todo eso con programaciÃ³n?

Â¡Bastante! La idea es tomar ese proceso y hacer algunas analogÃ­as ğŸ”„:  

 El metal serÃ­a el problema ğŸ§©. Queremos buscar el estado Ã³ptimo ğŸ¥‡.  
 Los Ã¡tomos serÃ­an cada uno de los estados ğŸ”¢.


Entonces, teniendo eso:  

 Al subir la temperatura ğŸ”¥ vamos a aceptar cualquier tipo de estado prÃ¡cticamente.  
 Porque en este contexto, la temperatura define el grado de libertad con el que vas a probar estados ğŸ².  
 A medida que baje la temperatura, vas a ser mÃ¡s conservador ğŸ§Š.


Vas comparando con una funciÃ³n que es la que vas a optimizar ğŸ“ˆ.  
Para tomar un estado, compruebas si es mejor que el que ya tenÃ­as âœ….  
En caso de que no, vas a darle un chance segÃºn una tolerancia ğŸ¯.  
En este caso usan una distribuciÃ³n llamada distribuciÃ³n de Boltzmann ğŸ“Š,  
donde evalÃºan esa tolerancia segÃºn esta expresiÃ³n:
e^(-delta/T)


 delta es la diferencia entre lo que da la funciÃ³n en el nuevo estado y en el actual ğŸ”.


Perfectamente puede ser otra funciÃ³n, pero esta es la que mÃ¡s se relaciona al fenÃ³meno fÃ­sico ğŸ§ .  
Lo importante es que sea una funciÃ³n que crezca al inicio y vaya decreciendo lentamente hasta hacer una asÃ­ntota ğŸ“‰.  
TambiÃ©n importante que la funciÃ³n estÃ© entre 0 y 1 ğŸ”¢.  
SegÃºn esa tolerancia, vas a probar quedarte con ese valor ğŸ§ª.

ğŸ§© Vale, muchas vueltas y Â¿dÃ³nde estÃ¡ el sudoku?

Para aterrizar con un ejemplo real vamos a usar el sudoku ğŸ§ .  
Primeramente, resolver un sudoku no es una funciÃ³n que se pueda optimizar directamente. Hay que ponerse creativos ğŸ¨.

Lo que se puede hacer es contar la cantidad de errores âŒ.  
Si yo pongo un nÃºmero, no puede ser cualquier nÃºmero.  
Tiene que evitar repetirse en la misma columna ğŸ“Š, fila ğŸ“ˆ y en el cuadrante ğŸ“¦ donde pusiste ese nÃºmero.  
Entonces, si el nÃºmero que colocas en una casilla no es el que va... Â¡pues tienes un error! ğŸš«

Entonces conseguido âœ…: ya convertimos el problema de resolver el sudoku en un problema de optimizaciÃ³n.  
Â¿CÃ³mo minimizar la cantidad de errores? ğŸ¯

Ahora, teniendo la funciÃ³n a optimizar, queda crear los estados o vecinos (tambiÃ©n se les dice asÃ­) ğŸ¤.

En este caso hay que usar la aleatoriedad ğŸ²:  

 Primero se rellena el tablero con nÃºmeros arbitrarios ğŸ”¢, no importa que uno se equivoque.  
 Ya ese es un estado y un punto de partida ğŸ›«.  
 Ahora, para generar un vecino, se buscan todas las casillas que se pueden modificar ğŸ§®, en las que no vino un nÃºmero por defecto  
 se toma una posiciÃ³n cualquiera y se le pone un nÃºmero ğŸ”„.
Ya con eso se tiene todo listo âœ… solo queda subirle la temperatura ğŸ”¥ y a que se cocine la soluciÃ³n ğŸ³.
```python
def solveSudoku(mat, time_step=0):
    temperature = 100
    initial = init(mat)
    score = cost_function(initial)
    i = 1
    voids = modified(mat)
    print_sudoku(initial)
    print('\n')
    if time_step != 0:
        time.sleep(time_step)
    while score != 0 and temperature > 0.1:
        neighbor = generate_neighbor(initial, voids)
        print_sudoku(neighbor)
        print('\n')
        if time_step != 0:
            time.sleep(time_step)
        neighbor_score = cost_function(neighbor)

        delta = score - neighbor_score
        print(f"delta: {delta} temperature: {temperature}")
        if delta > 0 or random.random() < exp(delta / temperature):
            initial = neighbor
            score = neighbor_score

        temperature *= 0.999
        i += 1
        if i == 1000:
            initial = init(mat)
    return initial
```

AcÃ¡ por ejemplo se prueba con 100, puede ser con cualquier otro nÃºmero.  
Entonces normalmente en estos algoritmos se prueban varias iteraciones ğŸ” porque no se sabe a veces cuÃ¡ndo es que se debe parar. Â¡No es el caso! ğŸ˜ porque aquÃ­ estamos seguros que el estado final es cuando lleguemos a 0 ğŸ¯.


Ahora la idea es que cuando se resten las funciones se mira si es mayor, como ya restamos podemos comparar si delta dio mayor que 0 o no.  
Y si no, pues todavÃ­a estÃ¡ la opciÃ³n de darle un chance ğŸ².

El valor de 0.999 con el que se baja la temperatura es ajustable ğŸ”§. Puedes probar con cualquier otro nÃºmero para ver cÃ³mo afecta el rendimiento.


Y eso serÃ­a todo de cÃ³mo funciona el algoritmo de recocido simulado ğŸ§ŠğŸ”¥.  
Es un algoritmo que depende mucho de si se tiene mucha informaciÃ³n ğŸ“š porque es bastante lento en cuanto a iteraciones ğŸ¢.  
Si lo comprueban, es mÃ¡s lento que el mismo de backtracking ğŸ§ .  
Es mÃ¡s Ãºtil cuando se tiene una funciÃ³n muy complicada de llegar a un punto, o si te interesa llegar a un punto Ã³ptimo pero tampoco tanto, ya que usa mucha aleatoriedad ğŸ°.

TambiÃ©n es importante cÃ³mo ajustas la temperatura ğŸŒ¡ï¸, cada cuÃ¡nto baja â³, cÃ³mo buscas los diferentes estados ğŸ§­ â€” todo eso influye en la cantidad de iteraciones que termina haciendo tu algoritmo de recocido â™»ï¸.
